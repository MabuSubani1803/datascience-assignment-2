{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "#importing dataset\n",
    "df=pd.read_csv('D:/Assignment and data set/amazon.csv')\n",
    "df\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df['reviewerName']\n",
    "\n",
    "df['reviewText']\n",
    "\n",
    "df.drop(columns='Unnamed: 0',inplace=True)\n",
    "df\n",
    "\n",
    "X=df['reviewText']\n",
    "X\n",
    "\n",
    "df.info()\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_float_values(reviewText):\n",
    "    # Ensure 'text' is a string\n",
    "    if not isinstance(reviewText, str):\n",
    "        return reviewText  # If it's not a string, return it as is (pass-through)\n",
    "\n",
    "    # Regular expression pattern to match float values\n",
    "    float_pattern = r'\\b\\d+\\.\\d+\\b|\\b\\d+\\b'\n",
    "\n",
    "    # Use the sub() method to replace float values with an empty string\n",
    "    cleaned_text = re.sub(float_pattern, '', reviewText)\n",
    "\n",
    "    return cleaned_text\n",
    "df['reviewText'] = df['reviewText'].apply(remove_float_values)\n",
    "\n",
    "\n",
    "# remove both the leading and the trailing characters\n",
    "def strip_text(reviewText):\n",
    "    # Check if the value is a string before applying the strip() method\n",
    "    return reviewText.strip() if isinstance(reviewText, str) else reviewText\n",
    "# Apply the strip_text function to the 'reviewText' column\n",
    "df['reviewText'] = df['reviewText'].apply(strip_text)\n",
    "\n",
    "\n",
    "#removes empty strings, because they are considered in Python as False\n",
    "df=[Text for Text in df if Text]\n",
    "\n",
    "\n",
    "df=[reviewText for reviewText in df if reviewText]\n",
    "df[0:10]\n",
    "df\n",
    "# Joining the list into one string/text\n",
    "text = ' '.join(df)\n",
    "text\n",
    "type(text)\n",
    "len(text)\n",
    "\n",
    "#Generate wordcloud\n",
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "# Define a function to plot word cloud\n",
    "def plot_cloud(wordcloud):\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(15, 30))\n",
    "    # Display image\n",
    "    plt.imshow(wordcloud) \n",
    "    # No axis details\n",
    "    plt.axis(\"off\");\n",
    "\n",
    "# Generate wordcloud\n",
    "type(STOPWORDS)\n",
    "stopwords = STOPWORDS\n",
    "len(stopwords)\n",
    "text\n",
    "type(text)\n",
    "\n",
    "# Plot\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, background_color='black', max_words=100,colormap='Set2',stopwords=stopwords).generate(text)\n",
    "plot_cloud(wordcloud)\n",
    "\n",
    "#text preprocessing start \n",
    "#Punctuation\n",
    "import string # special operations on strings\n",
    "no_punc_text = text.translate(str.maketrans('', '', string.punctuation)) \n",
    "no_punc_text\n",
    "\n",
    "#Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "text_tokens = word_tokenize(no_punc_text)\n",
    "len(text_tokens)\n",
    "print(text_tokens[0:50])\n",
    "\n",
    "#Remove stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "my_stop_words = stopwords.words('english')\n",
    "\n",
    "no_stop_tokens = [word for word in text_tokens if not word in my_stop_words]\n",
    "len(no_stop_tokens)\n",
    "print(no_stop_tokens[0:40])\n",
    "\n",
    "# joining the words in to single document\n",
    "doc = ' '.join(my_stop_words)\n",
    "doc\n",
    "print(doc[0:40])\n",
    "\n",
    "\n",
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "Lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmas = []\n",
    "for token in doc.split():\n",
    "    lemmas.append(Lemmatizer.lemmatize(token))\n",
    "\n",
    "print(lemmas)\n",
    "type(lemmas)\n",
    "\n",
    "#text preprocessing end\n",
    "\n",
    "#feature extraction start\n",
    "#how we converted in features\n",
    "#Feature Extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(lemmas)\n",
    "X\n",
    "\n",
    "# every word and its position in the X\n",
    "pd.DataFrame.from_records([vectorizer.vocabulary_]).T.sort_values(0,ascending=True).head(30)\n",
    "pd.DataFrame.from_records([vectorizer.vocabulary_]).T.sort_values(0,ascending=True)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.get_feature_names_out()[0:11])\n",
    "print(vectorizer.get_feature_names_out()[50:100])\n",
    "print(X.toarray()[50:100])\n",
    "\n",
    "#feature extraction end\n",
    "\n",
    "#identifying combination of words, bigram,s trigrams\n",
    "#Let's see how can bigrams and trigrams can be included here\n",
    "#Bigram\n",
    "vectorizer_ngram_range = CountVectorizer(analyzer='word',ngram_range=(1,1),max_features = 120)\n",
    "bow_matrix_ngram =vectorizer_ngram_range.fit_transform(df)\n",
    "bow_matrix_ngram\n",
    "type(df)\n",
    "\n",
    "print(vectorizer_ngram_range.get_feature_names_out())\n",
    "print(bow_matrix_ngram.toarray())\n",
    "\n",
    "print(vectorizer_ngram_range.get_feature_names_out())\n",
    "w1 = list(vectorizer_ngram_range.get_feature_names_out())\n",
    "type(w1)\n",
    "w2 = ' '.join(w1)\n",
    "w2\n",
    "type(w2)\n",
    "\n",
    "stopwords = set([word.lower() for word in STOPWORDS])\n",
    "\n",
    "# Generate WordCloud\n",
    "wordcloud = WordCloud(width=3000, height=2000, background_color='black', max_words=100, colormap='Set2', stopwords=stopwords).generate(w2)\n",
    "plt.figure(figsize=(15, 30))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "#Trigram\n",
    "vectorizer_ngram_range = CountVectorizer(analyzer='word',ngram_range=(1,2),max_features = 100)\n",
    "bow_matrix_ngram =vectorizer_ngram_range.fit_transform(df)\n",
    "bow_matrix_ngram\n",
    "\n",
    "print(vectorizer_ngram_range.get_feature_names_out())\n",
    "print(bow_matrix_ngram.toarray())\n",
    "\n",
    "w3 = list(vectorizer_ngram_range.get_feature_names_out())\n",
    "w3\n",
    "w4 = ' '.join(w3)\n",
    "w4\n",
    "stopwords = set([word.lower() for word in STOPWORDS])\n",
    "\n",
    "# Generate WordCloud\n",
    "wordcloud = WordCloud(width=3000, height=2000, background_color='black', max_words=100, colormap='Set2', stopwords=stopwords).generate(w4)\n",
    "plt.figure(figsize=(15, 30))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Emotion Minning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('D:/Assignment and data set/amazon.csv')\n",
    "df\n",
    "\n",
    "#installing textblob\n",
    "pip install textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "def get_sentiment(text):\n",
    "    text = str(text)\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "sentiment = get_sentiment(text)\n",
    "\n",
    "# Example usage:\n",
    "text = df['reviewText']\n",
    "sentiment = get_sentiment(text)\n",
    "print(\"Sentiment:\", sentiment)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

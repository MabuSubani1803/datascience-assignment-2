{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#Import Dataset\n",
    "df=pd.read_csv('D:/Assignment and data set/Elon_musk.csv',encoding='latin')\n",
    "df.drop(columns='Unnamed: 0',inplace=True)\n",
    "df\n",
    "X=df['Text']\n",
    "\n",
    "\n",
    "# apply text preprocessing\n",
    "df['Text'] = df.Text.map(lambda x : x.lower())\n",
    "df['Text']\n",
    "\n",
    "# remove both the leading and the trailing characters\n",
    "df=[Text.strip() for Text in df.Text] \n",
    "\n",
    "#removes empty strings, because they are considered in Python as False\n",
    "df=[Text for Text in df if Text]\n",
    "df[0:10]\n",
    "\n",
    "# Joining the list into one string/text\n",
    "text = ' '.join(df)\n",
    "text\n",
    "type(text)\n",
    "len(text)\n",
    "\n",
    "\n",
    "#Generate wordcloud\n",
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "# Define a function to plot word cloud\n",
    "def plot_cloud(wordcloud):\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(15, 30))\n",
    "    # Display image\n",
    "    plt.imshow(wordcloud) \n",
    "    # No axis details\n",
    "    plt.axis(\"off\");\n",
    "\n",
    "# Generate wordcloud\n",
    "type(STOPWORDS)\n",
    "stopwords = STOPWORDS\n",
    "len(stopwords)\n",
    "text\n",
    "type(text)\n",
    "\n",
    "# Plot\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, background_color='black', max_words=100,colormap='Set2',stopwords=stopwords).generate(text)\n",
    "plot_cloud(wordcloud)\n",
    "\n",
    "#text preprocessing start\n",
    "#Punctuation\n",
    "import string # special operations on strings\n",
    "no_punc_text = text.translate(str.maketrans('', '', string.punctuation)) \n",
    "no_punc_text\n",
    "\n",
    "#Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "text_tokens = word_tokenize(no_punc_text)\n",
    "len(text_tokens)\n",
    "print(text_tokens[0:50])\n",
    "\n",
    "#Remove stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "my_stop_words = stopwords.words('english')\n",
    "\n",
    "no_stop_tokens = [word for word in text_tokens if not word in my_stop_words]\n",
    "len(no_stop_tokens)\n",
    "print(no_stop_tokens[0:40])\n",
    "\n",
    "# joining the words in to single document\n",
    "doc = ' '.join(my_stop_words)\n",
    "doc\n",
    "print(doc[0:40])\n",
    "\n",
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "Lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmas = []\n",
    "for token in doc.split():\n",
    "    lemmas.append(Lemmatizer.lemmatize(token))\n",
    "\n",
    "print(lemmas)\n",
    "type(lemmas)\n",
    "\n",
    "#text preprocessing end\n",
    "#feature extraction start\n",
    "# how we converted in features\n",
    "#Feature Extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(lemmas)\n",
    "X\n",
    "\n",
    "# every word and its position in the X\n",
    "pd.DataFrame.from_records([vectorizer.vocabulary_]).T.sort_values(0,ascending=True).head(30)\n",
    "pd.DataFrame.from_records([vectorizer.vocabulary_]).T.sort_values(0,ascending=True)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.get_feature_names_out()[0:11])\n",
    "print(vectorizer.get_feature_names_out()[50:100])\n",
    "print(X.toarray()[50:100])\n",
    "\n",
    "#feature extraction end\n",
    "\n",
    "#identifying combination of words, bigram,s trigrams\n",
    "#Let's see how can bigrams and trigrams can be included here\n",
    "#Bigram\n",
    "vectorizer_ngram_range = CountVectorizer(analyzer='word',ngram_range=(1,1),max_features = 120)\n",
    "bow_matrix_ngram =vectorizer_ngram_range.fit_transform(df)\n",
    "bow_matrix_ngram\n",
    "type(df)\n",
    "\n",
    "print(vectorizer_ngram_range.get_feature_names_out())\n",
    "print(bow_matrix_ngram.toarray())\n",
    "\n",
    "print(vectorizer_ngram_range.get_feature_names_out())\n",
    "w1 = list(vectorizer_ngram_range.get_feature_names_out())\n",
    "type(w1)\n",
    "w2 = ' '.join(w1)\n",
    "w2\n",
    "type(w2)\n",
    "stopwords_list = set(stopwords.words('english'))\n",
    "#Plot\n",
    "wordcloud = WordCloud(\n",
    "    width=3000,\n",
    "    height=2000,\n",
    "    background_color='black',\n",
    "    max_words=120,\n",
    "    colormap='Set2',\n",
    "    stopwords=stopwords_list\n",
    ").generate(w2)\n",
    "plot_cloud(wordcloud)\n",
    "\n",
    "#Trigram\n",
    "vectorizer_ngram_range = CountVectorizer(analyzer='word',ngram_range=(2,2),max_features = 100)\n",
    "bow_matrix_ngram =vectorizer_ngram_range.fit_transform(df)\n",
    "bow_matrix_ngram\n",
    "\n",
    "print(vectorizer_ngram_range.get_feature_names_out())\n",
    "print(bow_matrix_ngram.toarray())\n",
    "\n",
    "w3 = list(vectorizer_ngram_range.get_feature_names_out())\n",
    "w3\n",
    "w4 = ' '.join(w3)\n",
    "w4\n",
    "stopwords_list = set(stopwords.words('english'))\n",
    "#Plot\n",
    "wordcloud = WordCloud(\n",
    "    width=3000,\n",
    "    height=2000,\n",
    "    background_color='black',\n",
    "    max_words=120,\n",
    "    colormap='Set2',\n",
    "    stopwords=stopwords_list\n",
    ").generate(w4)\n",
    "plot_cloud(wordcloud)\n",
    "\n",
    "\n",
    "\n",
    "#Sentiment Analysis\n",
    "#Import Dataset\n",
    "df=pd.read_csv('D:/Assignment and data set/Elon_musk.csv',encoding='latin')\n",
    "df.drop(columns='Unnamed: 0',inplace=True)\n",
    "df\n",
    "X=df['Text']\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "df['Sentiment'] = df['Text'].apply(lambda Text: analyzer.polarity_scores(Text)['compound'])\n",
    "\n",
    "#Analyze Results\n",
    "avg_sentiment = df['Sentiment'].mean()\n",
    "positive_tweets = df[df['Sentiment'] > 0]\n",
    "negative_tweets = df[df['Sentiment'] < 0]\n",
    "neutral_tweets = df[df['Sentiment'] == 0]\n",
    "\n",
    "print(\"Average Sentiment Score:\", avg_sentiment)\n",
    "print(\"Number of Positive Tweets:\", len(positive_tweets))\n",
    "print(\"Number of Negative Tweets:\", len(negative_tweets))\n",
    "print(\"Number of Neutral Tweets:\", len(neutral_tweets))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

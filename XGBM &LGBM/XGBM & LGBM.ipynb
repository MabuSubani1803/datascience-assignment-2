{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the Titanic datasets\n",
    "train_df = pd.read_csv('C:/Users/HP PROBOOK/Downloads/XGBM & LGBM/XGBM & LGBM/Titanic_train.csv')\n",
    "test_df = pd.read_csv('C:/Users/HP PROBOOK/Downloads/XGBM & LGBM/XGBM & LGBM/Titanic_test.csv')\n",
    "\n",
    "# Preprocessing the 'Name' column\n",
    "train_df['Title'] = train_df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "test_df['Title'] = test_df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "# Group rare titles into a common category\n",
    "train_df['Title'] = train_df['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "test_df['Title'] = test_df['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "# Map titles to numeric values\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "train_df['Title'] = train_df['Title'].map(title_mapping)\n",
    "test_df['Title'] = test_df['Title'].map(title_mapping)\n",
    "\n",
    "# Drop the 'Name' column\n",
    "train_df.drop(['Name'], axis=1, inplace=True)\n",
    "test_df.drop(['Name'], axis=1, inplace=True)\n",
    "\n",
    "# Drop the 'Ticket' and 'Cabin' columns\n",
    "train_df.drop(['Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "test_df.drop(['Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "\n",
    "# Impute missing values in 'Age' column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "train_df['Age'] = imputer.fit_transform(train_df[['Age']])\n",
    "test_df['Age'] = imputer.transform(test_df[['Age']])\n",
    "\n",
    "# Encode categorical variable 'Sex'\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['Sex'] = label_encoder.fit_transform(train_df['Sex'])\n",
    "test_df['Sex'] = label_encoder.transform(test_df['Sex'])\n",
    "\n",
    "# Impute missing values in 'Embarked' column\n",
    "train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True)\n",
    "test_df['Embarked'].fillna(test_df['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# One-Hot Encoding for 'Embarked' column\n",
    "onehot_encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "encoded_features = onehot_encoder.fit_transform(train_df[['Embarked']])\n",
    "train_df_encoded = pd.concat([train_df, pd.DataFrame(encoded_features, columns=onehot_encoder.get_feature_names_out(['Embarked']))], axis=1)\n",
    "train_df = train_df_encoded.drop(columns=['Embarked'])\n",
    "\n",
    "encoded_features = onehot_encoder.transform(test_df[['Embarked']])\n",
    "test_df_encoded = pd.concat([test_df, pd.DataFrame(encoded_features, columns=onehot_encoder.get_feature_names_out(['Embarked']))], axis=1)\n",
    "test_df = test_df_encoded.drop(columns=['Embarked'])\n",
    "\n",
    "# Building Predictive Models\n",
    "\n",
    "# Split the preprocessed training dataset into training and validation sets\n",
    "X_train = train_df.drop(columns=['Survived'])\n",
    "y_train = train_df['Survived']\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build predictive models using LightGBM and XGBoost algorithms\n",
    "\n",
    "# LightGBM model\n",
    "lgb_model = lgb.LGBMClassifier()\n",
    "lgb_model.fit(X_train_split, y_train_split)\n",
    "lgb_pred = lgb_model.predict(X_val_split)\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(X_train_split, y_train_split)\n",
    "xgb_pred = xgb_model.predict(X_val_split)\n",
    "\n",
    "# Evaluate model performance\n",
    "lgb_accuracy = accuracy_score(y_val_split, lgb_pred)\n",
    "lgb_precision = precision_score(y_val_split, lgb_pred)\n",
    "lgb_recall = recall_score(y_val_split, lgb_pred)\n",
    "lgb_f1 = f1_score(y_val_split, lgb_pred)\n",
    "\n",
    "xgb_accuracy = accuracy_score(y_val_split, xgb_pred)\n",
    "xgb_precision = precision_score(y_val_split, xgb_pred)\n",
    "xgb_recall = recall_score(y_val_split, xgb_pred)\n",
    "xgb_f1 = f1_score(y_val_split, xgb_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"LightGBM Metrics:\")\n",
    "print(f\"Accuracy: {lgb_accuracy:.2f}\")\n",
    "print(f\"Precision: {lgb_precision:.2f}\")\n",
    "print(f\"Recall: {lgb_recall:.2f}\")\n",
    "print(f\"F1-score: {lgb_f1:.2f}\")\n",
    "\n",
    "print(\"\\nXGBoost Metrics:\")\n",
    "print(f\"Accuracy: {xgb_accuracy:.2f}\")\n",
    "print(f\"Precision: {xgb_precision:.2f}\")\n",
    "print(f\"Recall: {xgb_recall:.2f}\")\n",
    "print(f\"F1-score: {xgb_f1:.2f}\")\n",
    "\n",
    "# Comparative Analysis\n",
    "# Comparing performance metrics of LightGBM and XGBoost models\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Model': ['LightGBM', 'XGBoost'],\n",
    "    'Accuracy': [lgb_accuracy, xgb_accuracy],\n",
    "    'Precision': [lgb_precision, xgb_precision],\n",
    "    'Recall': [lgb_recall, xgb_recall],\n",
    "    'F1-score': [lgb_f1, xgb_f1]\n",
    "})\n",
    "\n",
    "print(\"\\nComparative Analysis:\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Visualize and interpret the results to identify the strengths and weaknesses of each algorithm\n",
    "metrics_df.set_index('Model').plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Comparison of Model Performance Metrics')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Metrics')\n",
    "plt.show()\n",
    "\n",
    "# A brief report summarizing the comparative analysis results and practical implications\n",
    "print(\"\\nSummary Report:\")\n",
    "print(\"Both LightGBM and XGBoost models were trained and evaluated on the Titanic dataset.\")\n",
    "print(\"Comparing their performance metrics, we observe that:\")\n",
    "print(\"- LightGBM achieved higher accuracy, precision, and F1-score compared to XGBoost.\")\n",
    "print(\"- XGBoost had slightly higher recall compared to LightGBM.\")\n",
    "print(\"Overall, LightGBM outperformed XGBoost in terms of predictive performance on this dataset.\")\n",
    "print(\"However, it's essential to consider other factors such as training time, interpretability, and model complexity \"\n",
    "      \"when choosing between these algorithms for practical applications.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Fri Jul 28 23:30:48 2023\n",
    "\"\"\"\n",
    "# improting pandas and numpy \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#importing dataset\n",
    "wine_data = pd.read_csv(\"D:/Assignment and data set/wine.csv\")\n",
    "wine_data\n",
    "\n",
    "\n",
    "#Exploratory Data Analysis\n",
    "wine_data.head()\n",
    "\n",
    "wine_data.tail()\n",
    "\n",
    "wine_data.isnull().sum()\n",
    "\n",
    "wine_data.shape\n",
    "\n",
    "\n",
    "wine_data.columns\n",
    "\n",
    "list(wine_data)\n",
    "\n",
    "\n",
    "#Descriptive Statistics\n",
    "wine_data.describe()\n",
    "#Looking for some statistical information about each feature, we can see that the features have very diferrent scales\n",
    "wine_data.info()\n",
    "\n",
    "#Exploratory Data Analysis\n",
    "#Checking the skewness of our dataset.\n",
    "wine_data.skew()\n",
    "\"\"\"\n",
    "1.A normally distribuited data has a skewness close to zero.\n",
    "2.Skewness greather than zero means that there is more weight in the left side of the data.\n",
    "3.In another hand, skewness smaller than 0 means that there is more weight in the right side of the data\"\"\"\n",
    "\n",
    "#Data visualization\n",
    "#importing matplotlib and seabron\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "sn.pairplot(wine_data)\n",
    "plt.show()\n",
    "# Outliers Detection\n",
    "wine_data.plot( kind = 'box', subplots = True, layout = (4,4), sharex = False, sharey = False,color='black')\n",
    "plt.show()\n",
    "\n",
    "sn.pairplot(wine_data,palette=\"dark\")\n",
    "#correlation heatmap\n",
    "f,ax = plt.subplots(figsize=(18,12))\n",
    "sn.heatmap(wine_data.corr(), annot=True, linewidths =.5, fmt ='.1f',ax=ax)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "The is some Unique points in this correlation matrix:\n",
    "1.Phenols and Flavanoids is positively correlated, Dilution and Proanthocyanins\n",
    "2.Flavanoids is positively correlated with Proanthocyanins and Dilution\n",
    "3.Dilution is positively correlated with Hue\n",
    "4.Alcohol is positively correlated with Proline\"\"\"\n",
    "#plotting scattter plot Between Phenols and Flavanoids.\n",
    "plt.scatter(x=wine_data['Phenols'], y=wine_data['Flavanoids'], color='blue',lw=0.1)\n",
    "plt.xlabel('Phenols')\n",
    "plt.ylabel('Flavanoids')\n",
    "plt.title('Data represented by the 2 strongest positively Correlated features',fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "#Data Preprocessing\n",
    "#Applying Standard Scaler on the Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "std = ss.fit_transform(wine_data)\n",
    "std.shape\n",
    "\n",
    "\n",
    "# Principal Compound Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "PC = pca.fit_transform(std)\n",
    "\n",
    "### Creating a data frame to observe the variance\n",
    "df = pd.DataFrame(pca.explained_variance_ratio_)\n",
    "df\n",
    "\n",
    "### Taking first 3 principal components and creaating a dataframe\n",
    "pca_df = pd.DataFrame(data=PC, columns=['PC1', 'PC2', 'PC3'])\n",
    "pca_df\n",
    "\n",
    "\n",
    "#PCA plot in 2D\n",
    "# Figure size\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Scatterplot\n",
    "plt.scatter(pca_df.iloc[:,0], pca_df.iloc[:,1], s=40)\n",
    "plt.title('PCA plot in 2D using Strongest Principle Components')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "\n",
    "\n",
    "\n",
    "#AgglomerativeClustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean')\n",
    "Y = cluster.fit_predict(pca_df)\n",
    "Y\n",
    "#creating Y dataframe\n",
    "Y_new = pd.DataFrame(Y)  \n",
    "# Y value counts\n",
    "Y_new.value_counts()  # Y value counts\n",
    "\n",
    "# Initializing KMeans clustering\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3,n_init=20)\n",
    "kmeans = kmeans.fit(pca_df) # Fitting with inputs\n",
    "# Predicting the clusters\n",
    "Y = kmeans.predict(pca_df)\n",
    "Y_new = pd.DataFrame(Y)  ### creating Y dataframe\n",
    "Y_new.value_counts()  ### Y value counts\n",
    "\n",
    "#### Total with in centroid sum of squares \n",
    "kmeans.inertia_\n",
    "clust = []\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_clusters=i,random_state=0)\n",
    "    kmeans.fit(pca_df)\n",
    "    clust.append(kmeans.inertia_)\n",
    "\n",
    "##### Elbow method \n",
    "\n",
    "plt.scatter(x=range(1,11), y=clust,color='red')\n",
    "plt.plot(range(1,11), clust,color='black')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('inertial values')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Comparing between PCA-based clusters with original \"Type\" column\n",
    "#Using AgglomerativeClustering\n",
    "wine_data['PCA_Cluster'] = Y_new \n",
    "\n",
    "#### Using groupby function for Type and PCA_Cluster for comparing average \n",
    "cluster_comparison = wine_data.groupby(['Type', 'PCA_Cluster']).mean()\n",
    "cluster_comparison\n",
    "\n",
    "#Heirarchical cluster\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "lk = hierarchy.linkage(PC, method='complete')\n",
    "dendro = hierarchy.dendrogram(lk)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
